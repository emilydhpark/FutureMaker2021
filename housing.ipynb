{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "housing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqNV00eIZulhnuAAzmT/0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlapark/FutureMaker2021/blob/main/housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "ASkUTL8M5JE5",
        "outputId": "b8255f86-e788-4058-806e-73b22c876fc0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# This just means that if I want to refer to code in the package ‘pandas’, I’ll refer to it with the name pd.\n",
        "# We then read in the CSV file by running this line of code:\n",
        "\n",
        "df = pd.read_csv('housepricedata.csv')\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# This line of code means that we will read the csv file ‘housepricedata.csv’\n",
        "# (which should be in the same directory as your notebook) and store it in the variable ‘df’.\n",
        "# If we want to find out what is in df, simply type df into the grey box and click Alt-Enter:\n",
        "\n",
        "df\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>AboveMedianPrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8450</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>856</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>548</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9600</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1262</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>460</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11250</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>920</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>608</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9550</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>756</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>642</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14260</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1145</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>836</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>7917</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>953</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>460</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>13175</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1542</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>9042</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>1152</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>252</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>9717</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1078</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>240</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>9937</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>276</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      LotArea  OverallQual  ...  GarageArea  AboveMedianPrice\n",
              "0        8450            7  ...         548                 1\n",
              "1        9600            6  ...         460                 1\n",
              "2       11250            7  ...         608                 1\n",
              "3        9550            7  ...         642                 0\n",
              "4       14260            8  ...         836                 1\n",
              "...       ...          ...  ...         ...               ...\n",
              "1455     7917            6  ...         460                 1\n",
              "1456    13175            6  ...         500                 1\n",
              "1457     9042            7  ...         252                 1\n",
              "1458     9717            5  ...         240                 0\n",
              "1459     9937            5  ...         276                 0\n",
              "\n",
              "[1460 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ54jzy0-A55"
      },
      "source": [
        "\n",
        "Here, you can explore the data a little. We have our input features in the first ten columns:\n",
        "Number of Full Bathrooms\n",
        "Number of Half Bathrooms\n",
        "Number of Bedrooms above ground\n",
        "Total Number of Rooms above ground\n",
        "Number of Fireplaces\n",
        "Garage Area (in sq ft)\n",
        "In our last column, we have the feature that we would like to predict:\n",
        "\n",
        "Is the house price above the median or not? (1 for yes and 0 for no)\n",
        "Now that we’ve seen what our data looks like, we want to convert it into arrays for our machine to process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stpZd6C0-P34"
      },
      "source": [
        "dataset = df.values\n",
        "\n",
        "# To convert our dataframe into an array, we just store the values of df (by accessing df.values) into the variable ‘dataset’. To see what is inside this variable ‘dataset’, simply type ‘dataset’ into a grey box on your notebook and run the cell (Alt-Enter):\n",
        "\n",
        "dataset\n",
        "\n",
        "# We now split our dataset into input features (X) and the feature we wish to predict (Y). To do that split, we simply assign the first 10 columns of our array to a variable called X and the last column of our array to a variable called Y. The code to do the first assignment is this:\n",
        "\n",
        "X = dataset[:,0:10]\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcA2gBf9-dTx"
      },
      "source": [
        "This might look a bit weird, but let me explain what’s inside the square brackets. Everything before the comma refers to the rows of the array and everything after the comma refers to the columns of the arrays.\n",
        "\n",
        "Since we’re not splitting up the rows, we put ‘:’ before the comma. This means to take all the rows in dataset and put it in X.\n",
        "\n",
        "We want to extract out the first 10 columns, and so the ‘0:10’ after the comma means take columns 0 to 9 and put it in X (we don’t include column 10). Our columns start from index 0, so the first 10 columns are really columns 0 to 9.\n",
        "\n",
        "We then assign the last column of our array to Y:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbBM5niN-ghU",
        "outputId": "bf4cabd6-8139-4c2d-bc12-14853c0c7b7a"
      },
      "source": [
        "Y = dataset[:,10]\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = min_max_scaler.fit_transform(X)\n",
        "\n",
        "X_scale\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0334198 , 0.66666667, 0.5       , ..., 0.5       , 0.        ,\n",
              "        0.3864598 ],\n",
              "       [0.03879502, 0.55555556, 0.875     , ..., 0.33333333, 0.33333333,\n",
              "        0.32440056],\n",
              "       [0.04650728, 0.66666667, 0.5       , ..., 0.33333333, 0.33333333,\n",
              "        0.42877292],\n",
              "       ...,\n",
              "       [0.03618687, 0.66666667, 1.        , ..., 0.58333333, 0.66666667,\n",
              "        0.17771509],\n",
              "       [0.03934189, 0.44444444, 0.625     , ..., 0.25      , 0.        ,\n",
              "        0.16925247],\n",
              "       [0.04037019, 0.44444444, 0.625     , ..., 0.33333333, 0.        ,\n",
              "        0.19464034]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Je75ahN-ufT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split your data:\n",
        "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)\n",
        "\n",
        "# This tells scikit-learn that your val_and_test size will be 30% of the overall dataset. The code will store the split data into the first four variables on the left of the equal sign as the variable names suggest.\n",
        "# Unfortunately, this function only helps us split our dataset into two. Since we want a separate validation set and test set, we can use the same function to do the split again on val_and_test:\n",
        "\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7ppqWhT-7Ga"
      },
      "source": [
        "The code above will split the val_and_test size equally to the validation set and the test set.\n",
        "\n",
        "In summary, we now have a total of six variables for our datasets we will use:\n",
        "\n",
        "X_train (10 input features, 70% of full dataset)\n",
        "X_val (10 input features, 15% of full dataset)\n",
        "X_test (10 input features, 15% of full dataset)\n",
        "Y_train (1 label, 70% of full dataset)\n",
        "Y_val (1 label, 15% of full dataset)\n",
        "Y_test (1 label, 15% of full dataset)\n",
        "If you want to see how the shapes of the arrays are for each of them (i.e. what dimensions they are), simply run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR2WGFxg-8ws",
        "outputId": "89f6fcc5-2e23-422b-ef39-87a8296fbaaa"
      },
      "source": [
        "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1022, 10) (219, 10) (219, 10) (1022,) (219,) (219,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_VJzoFo_B-D"
      },
      "source": [
        "As you can see, the training set has 1022 data points while the validation and test set has 219 data points each. The X variables have 10 input features, while the Y variables only has one feature to predict.\n",
        "\n",
        "And now, our data is finally ready! Phew!\n",
        "\n",
        "Summary: In processing the data, we’ve:\n",
        "\n",
        "Read in the CSV (comma separated values) file and convert them to arrays.\n",
        "Split our dataset into the input features and the label.\n",
        "Scale the data so that the input features have similar orders of magnitude.\n",
        "Split our dataset into the training set, the validation set and the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F-0X4Ncx_IJF",
        "outputId": "5be296b6-2ded-4336-ca7d-2015853743a4"
      },
      "source": [
        "# First, let’s import the necessary code from Keras:\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Then, we specify that in our Keras sequential model like this:\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(10,)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "# Configuring the model with these settings requires us to call the function model.compile, like this:\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training on the data is pretty straightforward and requires us to write one line of code:\n",
        "\n",
        "hist = model.fit(X_train, Y_train,\n",
        "          batch_size=32, epochs=100,\n",
        "          validation_data=(X_val, Y_val))\n",
        "\n",
        "model.evaluate(X_test, Y_test)[1]\n",
        "\n",
        "# Then, we want to visualize the training loss and the validation loss. To do so, run this snippet of code:\n",
        "\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='lower right')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.6994 - accuracy: 0.5060 - val_loss: 0.6886 - val_accuracy: 0.5160\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6869 - accuracy: 0.5241 - val_loss: 0.6812 - val_accuracy: 0.5753\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.6145 - val_loss: 0.6736 - val_accuracy: 0.7763\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.7613 - val_loss: 0.6667 - val_accuracy: 0.8082\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6670 - accuracy: 0.7770 - val_loss: 0.6603 - val_accuracy: 0.7991\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.7663 - val_loss: 0.6538 - val_accuracy: 0.7945\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.7700 - val_loss: 0.6466 - val_accuracy: 0.7991\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.7742 - val_loss: 0.6392 - val_accuracy: 0.7991\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6422 - accuracy: 0.7861 - val_loss: 0.6314 - val_accuracy: 0.8082\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6346 - accuracy: 0.7890 - val_loss: 0.6234 - val_accuracy: 0.8128\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6283 - accuracy: 0.8021 - val_loss: 0.6152 - val_accuracy: 0.8128\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.8146 - val_loss: 0.6066 - val_accuracy: 0.8174\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.8334 - val_loss: 0.5979 - val_accuracy: 0.8174\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6059 - accuracy: 0.8297 - val_loss: 0.5891 - val_accuracy: 0.8128\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.8224 - val_loss: 0.5798 - val_accuracy: 0.8174\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.8452 - val_loss: 0.5702 - val_accuracy: 0.8219\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.8428 - val_loss: 0.5603 - val_accuracy: 0.8356\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5685 - accuracy: 0.8284 - val_loss: 0.5504 - val_accuracy: 0.8356\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5645 - accuracy: 0.8257 - val_loss: 0.5401 - val_accuracy: 0.8356\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5450 - accuracy: 0.8392 - val_loss: 0.5293 - val_accuracy: 0.8539\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.8119 - val_loss: 0.5186 - val_accuracy: 0.8630\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5244 - accuracy: 0.8547 - val_loss: 0.5079 - val_accuracy: 0.8676\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.8620 - val_loss: 0.4966 - val_accuracy: 0.8721\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.8440 - val_loss: 0.4861 - val_accuracy: 0.8721\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.8641 - val_loss: 0.4751 - val_accuracy: 0.8721\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.8544 - val_loss: 0.4646 - val_accuracy: 0.8767\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.8459 - val_loss: 0.4542 - val_accuracy: 0.8721\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.8366 - val_loss: 0.4435 - val_accuracy: 0.8721\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.8517 - val_loss: 0.4336 - val_accuracy: 0.8721\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4436 - accuracy: 0.8665 - val_loss: 0.4240 - val_accuracy: 0.8767\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8653 - val_loss: 0.4157 - val_accuracy: 0.8676\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8706 - val_loss: 0.4065 - val_accuracy: 0.8767\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8657 - val_loss: 0.3981 - val_accuracy: 0.8767\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8363 - val_loss: 0.3893 - val_accuracy: 0.8767\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8773 - val_loss: 0.3819 - val_accuracy: 0.8767\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.8539 - val_loss: 0.3752 - val_accuracy: 0.8767\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3877 - accuracy: 0.8705 - val_loss: 0.3676 - val_accuracy: 0.8767\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8701 - val_loss: 0.3626 - val_accuracy: 0.8767\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.8547 - val_loss: 0.3556 - val_accuracy: 0.8767\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8798 - val_loss: 0.3517 - val_accuracy: 0.8767\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8470 - val_loss: 0.3457 - val_accuracy: 0.8767\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8681 - val_loss: 0.3418 - val_accuracy: 0.8767\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3627 - accuracy: 0.8711 - val_loss: 0.3365 - val_accuracy: 0.8767\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8721 - val_loss: 0.3328 - val_accuracy: 0.8767\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3370 - accuracy: 0.8768 - val_loss: 0.3296 - val_accuracy: 0.8813\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8611 - val_loss: 0.3256 - val_accuracy: 0.8813\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.8835 - val_loss: 0.3213 - val_accuracy: 0.8904\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.8480 - val_loss: 0.3188 - val_accuracy: 0.8904\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8668 - val_loss: 0.3152 - val_accuracy: 0.8904\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3300 - accuracy: 0.8784 - val_loss: 0.3125 - val_accuracy: 0.8904\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8796 - val_loss: 0.3103 - val_accuracy: 0.8950\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8631 - val_loss: 0.3082 - val_accuracy: 0.8950\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.8888 - val_loss: 0.3059 - val_accuracy: 0.8950\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8672 - val_loss: 0.3037 - val_accuracy: 0.8950\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3193 - accuracy: 0.8660 - val_loss: 0.3026 - val_accuracy: 0.8950\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8690 - val_loss: 0.3013 - val_accuracy: 0.8904\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8612 - val_loss: 0.2991 - val_accuracy: 0.8950\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3159 - accuracy: 0.8751 - val_loss: 0.2964 - val_accuracy: 0.8950\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.8721 - val_loss: 0.2970 - val_accuracy: 0.8904\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.8767 - val_loss: 0.2941 - val_accuracy: 0.8950\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.8696 - val_loss: 0.2942 - val_accuracy: 0.8904\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8748 - val_loss: 0.2912 - val_accuracy: 0.8950\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2985 - accuracy: 0.8821 - val_loss: 0.2898 - val_accuracy: 0.8950\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.8803 - val_loss: 0.2881 - val_accuracy: 0.8950\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.8627 - val_loss: 0.2871 - val_accuracy: 0.8950\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8642 - val_loss: 0.2860 - val_accuracy: 0.8995\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3264 - accuracy: 0.8575 - val_loss: 0.2850 - val_accuracy: 0.8995\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.8712 - val_loss: 0.2858 - val_accuracy: 0.8950\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8815 - val_loss: 0.2831 - val_accuracy: 0.8995\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3164 - accuracy: 0.8615 - val_loss: 0.2817 - val_accuracy: 0.8995\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3194 - accuracy: 0.8602 - val_loss: 0.2817 - val_accuracy: 0.8995\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8823 - val_loss: 0.2809 - val_accuracy: 0.8995\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8604 - val_loss: 0.2793 - val_accuracy: 0.8995\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.8573 - val_loss: 0.2798 - val_accuracy: 0.8950\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.8677 - val_loss: 0.2778 - val_accuracy: 0.8995\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8804 - val_loss: 0.2776 - val_accuracy: 0.8995\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.8921 - val_loss: 0.2771 - val_accuracy: 0.8995\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8796 - val_loss: 0.2759 - val_accuracy: 0.8995\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2962 - accuracy: 0.8739 - val_loss: 0.2782 - val_accuracy: 0.8995\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.9018 - val_loss: 0.2744 - val_accuracy: 0.8995\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2845 - accuracy: 0.8835 - val_loss: 0.2737 - val_accuracy: 0.8995\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8680 - val_loss: 0.2732 - val_accuracy: 0.8995\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.8876 - val_loss: 0.2729 - val_accuracy: 0.8995\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8669 - val_loss: 0.2722 - val_accuracy: 0.8995\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.8852 - val_loss: 0.2719 - val_accuracy: 0.8995\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.9002 - val_loss: 0.2709 - val_accuracy: 0.8995\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.8805 - val_loss: 0.2710 - val_accuracy: 0.8995\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2875 - accuracy: 0.8740 - val_loss: 0.2708 - val_accuracy: 0.8995\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.8741 - val_loss: 0.2693 - val_accuracy: 0.8995\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8764 - val_loss: 0.2711 - val_accuracy: 0.9041\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.8923 - val_loss: 0.2684 - val_accuracy: 0.8995\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8777 - val_loss: 0.2679 - val_accuracy: 0.8995\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2765 - accuracy: 0.8805 - val_loss: 0.2676 - val_accuracy: 0.8995\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8854 - val_loss: 0.2673 - val_accuracy: 0.8995\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8765 - val_loss: 0.2665 - val_accuracy: 0.8995\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2547 - accuracy: 0.8961 - val_loss: 0.2669 - val_accuracy: 0.9041\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3038 - accuracy: 0.8747 - val_loss: 0.2660 - val_accuracy: 0.8995\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8490 - val_loss: 0.2663 - val_accuracy: 0.9041\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.8638 - val_loss: 0.2652 - val_accuracy: 0.8995\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2773 - accuracy: 0.8753 - val_loss: 0.2648 - val_accuracy: 0.9041\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d09d47b4095e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'acc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbOHxaiL_o7c"
      },
      "source": [
        "Adding Regularization to our Neural Network\n",
        "For the sake of introducing regularization to our neural network, let’s formulate with a neural network that will badly overfit on our training set. We’ll call this Model 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzY_ls9J_rOg"
      },
      "source": [
        "model_2 = Sequential([\n",
        "    Dense(1000, activation='relu', input_shape=(10,)),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model_2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist_2 = model_2.fit(X_train, Y_train,\n",
        "          batch_size=32, epochs=100,\n",
        "          validation_data=(X_val, Y_val))\n",
        "\n",
        "plt.plot(hist_2.history['loss'])\n",
        "plt.plot(hist_2.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(hist_2.history['acc'])\n",
        "plt.plot(hist_2.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "from keras.layers import Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "model_3 = Sequential([\n",
        "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(10,)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n",
        "])\n",
        "\n",
        "model_3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "hist_3 = model_3.fit(X_train, Y_train,\n",
        "          batch_size=32, epochs=100,\n",
        "          validation_data=(X_val, Y_val))\n",
        "\n",
        "plt.plot(hist_3.history['loss'])\n",
        "plt.plot(hist_3.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.ylim(top=1.2, bottom=0)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(hist_3.history['acc'])\n",
        "plt.plot(hist_3.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}