# FutureMaker2021
### Day 1: July 6, 2021
#    - In this program, I hope to become more familiar with deep learning and languages that I have not used much, such as C++
#    - Although I have already done research in computer vision and deep learning, I hope the program can cover more in-depth conceptual informations.
#    - NumPy is one of the most popular Python libraries. It stands for Numerical Python and provides support for manipulating large, multi-dimensional arrays and matrices. Several of our activities within this curriculum will explore various NumPy arrays and tools.

### Day 2: July 7, 2021
#    - Dr. David Kong's talk was very inspiring, as he pushed everyone and helped me believe that everyone has a story -- I just have to learn to speak my unique experiences and passions in a strong, inspiring way
#    - Making a common goal and developing a realistic plan to achieve that goal was also something that he covered. His talk touched me so much that I even contacted him through Instagram afterwards.

### Day 3: July 8, 2021
#    - A supervised learning is where each data is labeled then processed to predict an outcome ("either regression or classification")
#    - A unsupervised learning means that the model uses algorithms "to analyze unlabeled data sets and draw patterns between them without the need of human intervension." (i.e. clustering, dimensionality reduction)

### Day 4: July 9, 2021
#    - Real world problem: classifying the exact location through a series of 360 degree images and videos; video classification; instant segmentation
#    - Deep learning -- the network will be trained through series of hundreds of images and videos

### Day 5-6 -- Weekend (catch-up days)

# Day 7: July 12, 2021
#    - Not video -- writing in the ReadMe section:
#         - A tensor is a general term that "encapsulates vectors and amtrices in any dimension"
#         - There has to be more than enough data to train an algorithm/network and produced accurate data and information
#         - The modern data is rarely limited to just 3 dimensions, and tensors provide a way to represent information and store data in a multimensional space.

# Day 8: July 13, 2021
#    - Use what you learned in the article above (i.e. Neuron, Weights, Bias, Functions, Sigmoid, Softmax, Input vs Output, etc) to develop a NN model using this Kaggle Notebook. If you need some scaffolding code to get started, check out the “Code” tab on the shared link and select a recent Gold/Silver medal-ed notebook. Feel free to examine different notebooks to see different model topologies and select one that you find the most interesting or even explore combining them
#    - ********** NEED TO FINISH *************

### Day 9: July 14, 2021
#    - Review this Kaggle tutorial. Extend the training data using the full training data from this MNIST Database. Evaluate the model performance (hint: look at the confusion matrices) applied to the same test-set in comparison to the performance reported in the original Kaggle Tutorial Notebook.
#    - ********** NEED TO FINISH *************

# Day 10: July 15, 2021
#    - [Missed this day]
#    - "An artificial neural network is a network of artificial neurons which take in input, perform algorithms, and output values which can solve complex problems that can normally only be performed by "intelligent" humans. The process of a neural network is as follows: input -> prediction -> calculate error -> tweak weights and biases -> repeat. The way input data flows through the neural network is that each node's input is multiplied by a weight and added to a bias. Each of these connections are summed and ran through a non-linear activation function. Non-linear activations are a way of adding non-linearity to a model in order to detect more complex patterns in the data and is a means of defining how a node's output will be expressed for the next layer. The method that the model becomes trained is through gradient descent, in which the weights and biases are tweaked in the direction that the cost function is approaching a local minimum. Forward propagation is the process of feeding input data forward through the neural network. Backward propagation is the process of using training the model using gradient descent."

# Day 11: July 16, 2021
#    - "A convolutional layer is much more specialized, and efficient, than a fully connected layer. In a fully connected layer each neuron is connected to every neuron in the previous layer, and each connection has it's own weight"
#    - "The reason why Convolutional Neural Networks (CNNs) do so much better than classic neural networks on images and videos is that the convolutional layers take advantage of inherent properties of images. Simple feedforward neural networks don't see any order in their inputs."

# Day 12-13 -- Weekend (catch-up days)

# Day 14: July 19, 2021
#    - Cost functions = calculation of how much error is produced from the model vs. the  target value
#    - mini-batches = variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients
#    - bias, variance = simplifying assumptions made by the model to make the target function easier to approximate; amount that the estimate of the target function will change given different training data
